{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python3-wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pycurl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 0: DATABASE TABLES CREATION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE EXTENSION pgcrypto;\n",
    "\n",
    "\n",
    "--\n",
    "-- Name: pub_review_paragraphs; Type: TABLE; Schema: public; Owner: postgres\n",
    "--\n",
    "\n",
    "CREATE TABLE public.pub_review_paragraphs (\n",
    "    id uuid DEFAULT public.gen_random_uuid() NOT NULL,\n",
    "    pub_id uuid NOT NULL,\n",
    "    paragraph_text text NOT NULL,\n",
    "    date_created timestamp with time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,\n",
    "    date_updated timestamp with time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,\n",
    "    username text NOT NULL\n",
    ");\n",
    "\n",
    "\n",
    "ALTER TABLE public.pub_review_paragraphs OWNER TO postgres;\n",
    "\n",
    "--\n",
    "-- Name: publication_urls; Type: TABLE; Schema: public; Owner: postgres\n",
    "--\n",
    "\n",
    "CREATE TABLE public.publication_urls (\n",
    "    id uuid DEFAULT public.gen_random_uuid() NOT NULL,\n",
    "    site_id uuid,\n",
    "    date_created timestamp with time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,\n",
    "    date_updated timestamp with time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,\n",
    "    username text NOT NULL,\n",
    "    pub_url text NOT NULL,\n",
    "    is_scraped boolean DEFAULT false NOT NULL\n",
    ");\n",
    "\n",
    "\n",
    "ALTER TABLE public.publication_urls OWNER TO postgres;\n",
    "\n",
    "--\n",
    "-- Name: site_review_urls; Type: TABLE; Schema: public; Owner: postgres\n",
    "--\n",
    "\n",
    "CREATE TABLE public.site_review_urls (\n",
    "    id uuid DEFAULT public.gen_random_uuid() NOT NULL,\n",
    "    site_url text NOT NULL,\n",
    "    date_created timestamp with time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,\n",
    "    date_updated timestamp with time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,\n",
    "    username text NOT NULL\n",
    ");\n",
    "\n",
    "\n",
    "ALTER TABLE public.site_review_urls OWNER TO postgres;\n",
    "\n",
    "--\n",
    "-- Name: pub_review_paragraphs pub_review_paragraphs_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres\n",
    "--\n",
    "\n",
    "ALTER TABLE ONLY public.pub_review_paragraphs\n",
    "    ADD CONSTRAINT pub_review_paragraphs_pkey PRIMARY KEY (id);\n",
    "\n",
    "\n",
    "--\n",
    "-- Name: publication_urls publication_urls_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres\n",
    "--\n",
    "\n",
    "ALTER TABLE ONLY public.publication_urls\n",
    "    ADD CONSTRAINT publication_urls_pkey PRIMARY KEY (id);\n",
    "\n",
    "\n",
    "--\n",
    "-- Name: site_review_urls site_review_urls_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres\n",
    "--\n",
    "\n",
    "ALTER TABLE ONLY public.site_review_urls\n",
    "    ADD CONSTRAINT site_review_urls_pkey PRIMARY KEY (id);\n",
    "\n",
    "\n",
    "--\n",
    "-- Name: pub_review_paragraphs fk_puburl_pubid; Type: FK CONSTRAINT; Schema: public; Owner: postgres\n",
    "--\n",
    "\n",
    "ALTER TABLE ONLY public.pub_review_paragraphs\n",
    "    ADD CONSTRAINT fk_puburl_pubid FOREIGN KEY (pub_id) REFERENCES public.publication_urls(id);\n",
    "\n",
    "\n",
    "--\n",
    "-- Name: publication_urls fk_puburl_siteid; Type: FK CONSTRAINT; Schema: public; Owner: postgres\n",
    "--\n",
    "\n",
    "ALTER TABLE ONLY public.publication_urls\n",
    "    ADD CONSTRAINT fk_puburl_siteid FOREIGN KEY (site_id) REFERENCES public.site_review_urls(id);\n",
    "\n",
    "\n",
    "\n",
    "https://starkandwayne.com/blog/uuid-primary-keys-in-postgresql/   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "# method to get a connection to local PostgreSQL database\n",
    "def get_conn_rds(): \n",
    "\n",
    "    try:\n",
    "        # connect to the postgresql database\n",
    "        print(\"Connecting to the PostgreSQL database...\")\n",
    "\n",
    "        connection = psycopg2.connect(user = \"postgres\",\n",
    "                                  password = \"9UhLiYiX\",   \n",
    "                                  host = \"twiviews.cco0dsf0nmqk.us-east-2.rds.amazonaws.com\",         # and hostname\n",
    "                                  port = \"5432\",\n",
    "                                  database = \"twiviews\")\n",
    "\n",
    "        return connection\n",
    "\n",
    "    except (Exception, psycopg2.Error) as error :\n",
    "        print (\"Error while connecting to PostgreSQL\", error)\n",
    "        \n",
    "conn = get_conn_rds()\n",
    "print(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import datetime\n",
    "\n",
    "## Insert into table : site_review_urls. \n",
    "## date_created, date_updated are set to default CURRENT_TIMESTAMP. No need to pass them in inserts anymore.\n",
    "\n",
    "def insert_to_tbl_site_review_urls(site_url, username):\n",
    "    try:\n",
    "        conn = get_conn_rds()\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # please note the change here. we longer need to pass date_created, date_updated. I have now set these\n",
    "        # to default value CURRENT_TIMESTAMP for inserts. \n",
    "        query_params_list = {'site_url': site_url,                              \n",
    "                             'username': username}\n",
    "        sql = '''\n",
    "                INSERT INTO public.\"site_review_urls\"(site_url, username)\n",
    "                        VALUES (%(site_url)s, %(username)s);\n",
    "                '''\n",
    "        \n",
    "        cur.execute(sql, query_params_list)\n",
    "\n",
    "        conn.commit()\n",
    "        \n",
    "        print(\"Successfully inserted a record\")\n",
    "\n",
    "        cur.close()\n",
    "        \n",
    "        print(\"Closing Connection.\")\n",
    "    \n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 - You get urls for all movies on page1 page2 and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import uuid\n",
    "import psycopg2\n",
    "\n",
    "links_page1 = []\n",
    "result_urls = []\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "site_movies_by_year_desc_url = 'https://www.metacritic.com/browse/movies/score/metascore/year/filtered?sort=desc'\n",
    "\n",
    "# webscraper method to first get all the urls on the above url where I'm filtering \"desc\" for current year 2020.\n",
    "# similarly, the above url can be changed to the year 2019. \n",
    "# The following method just scrapes all the review urls on page 1. I'll need to do the same for page2 and so on.\n",
    "def fetchandinsert_movie_releases_by_year_urls():\n",
    "    browser = webdriver.Chrome(\"/home/srini/pyprjs/scrpr/chromedriver\", options=options)\n",
    "    browser.get(site_movies_by_year_desc_url)\n",
    "    css_selector_metacritic_review_links = \"div.clamp-metascore\"\n",
    "    divs = browser.find_elements_by_css_selector(css_selector_metacritic_review_links)\n",
    "    for div in divs:\n",
    "        site_url = div.find_element_by_css_selector('a').get_attribute('href')\n",
    "        # inserts the scraped urls on the page into the database table :metacritics_review_urls\n",
    "        insert_to_tbl_site_review_urls(site_url, 'Srini')\n",
    "        links_page1.append(site_url)\n",
    "    return links_page1\n",
    "\n",
    "result_urls = fetchandinsert_movie_releases_by_year_urls()\n",
    "print(len(result_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2 - read the urls from the database and now for each of these urls - we'll need to get publication urls for \n",
    "#          critic reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.a SELECT URLS FROM DATABASE TABLE: metacritic_review_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "\n",
    "def select_from_tbl_site_review_urls():\n",
    "    \n",
    "    site_review_urls = dict()\n",
    "    \n",
    "    try:\n",
    "        conn = get_conn_rds()\n",
    "        cursor = conn.cursor()\n",
    "        select_query = \"select * from public.site_review_urls\"\n",
    "        cursor.execute(select_query)\n",
    "        print(\"Selecting rows from the table using cursor.fetchall\")\n",
    "        result_set = cursor.fetchall() \n",
    "   \n",
    "        for row in result_set:\n",
    "            site_review_urls[row[0]]= row[1]\n",
    "            \n",
    "       \n",
    "    except (Exception, psycopg2.Error) as error :\n",
    "        print (\"Error while fetching data from PostgreSQL\", error)\n",
    "\n",
    "    finally:\n",
    "    #closing database connection.\n",
    "        if(conn):\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            print(\"PostgreSQL connection is closed\")\n",
    "    \n",
    "    return site_review_urls\n",
    "\n",
    "\n",
    "result_dict = select_from_tbl_site_review_urls()\n",
    "for key, value in result_dict.items():\n",
    "    print(f\"{key} : {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.b INSERT INTO TABLE: publication_urls to be used by: fetchandinsert_publication_urls_per_metacritic_url. \n",
    "##     Please see 2.c.\n",
    "## date_created, date_updated are set to default CURRENT_TIMESTAMP. No need to pass them in inserts anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import datetime\n",
    "\n",
    "def insert_to_tbl_publication_urls(site_id, pub_url, username):\n",
    "    try:\n",
    "        conn = get_conn_rds()\n",
    "        cur = conn.cursor()\n",
    "        query_params_list = {'site_id' : site_id,\n",
    "                             'pub_url': pub_url,                              \n",
    "                             'username': username}\n",
    "        sql = '''\n",
    "                INSERT INTO public.\"publication_urls\"(site_id, pub_url, username)\n",
    "                        VALUES (%(site_id)s, %(pub_url)s, %(username)s);\n",
    "                '''\n",
    "        \n",
    "        cur.execute(sql, query_params_list)\n",
    "\n",
    "        conn.commit()\n",
    "        \n",
    "        print(\"Successfully inserted a record\")\n",
    "\n",
    "        cur.close()\n",
    "        \n",
    "        print(\"Closing Connection.\")\n",
    "    \n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STPE 3 - Method to fetch the publication urls per metacritic review url.\n",
    "##     Makes use of methods: 2.a and 2.b.\n",
    "##     Please note the time.sleep - we'll need to pause a while to not be aggressive.\n",
    "##     If scraping night time, these could be relaxed, say commenting inner for loop sleep \n",
    "##     and changing outer for loop sleep to 60 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "pub_links =[]\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "def fetchandinsert_publication_urls_per_site_url():\n",
    "    \n",
    "    site_review_urls = select_from_tbl_site_review_urls()  \n",
    "        \n",
    "    browser = webdriver.Chrome(\"/home/srini/pyprjs/scrpr/chromedriver\", options=options)\n",
    "    for site_id, site_url in site_review_urls.items():\n",
    "            browser.get(site_url)\n",
    "            webElem = browser.find_elements_by_css_selector(\"a.read_full\")\n",
    "            for elem in webElem:\n",
    "                pub_url = elem.get_attribute('href')\n",
    "                insert_to_tbl_publication_urls(site_id, pub_url, 'Srini')\n",
    "                pub_links.append(pub_href)\n",
    "                time.sleep(60)\n",
    "            time.sleep(120)\n",
    "        \n",
    "    \n",
    "fetchandinsert_publication_urls_per_site_url()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below two files are the csv exports for review of the data from above two tables: \n",
    "# metacritic_review_urls,\n",
    "# publication_urls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lrth *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility methods for STEP 4\n",
    "## 4.a\n",
    "\n",
    "def select_un_scraped_publication_urls():\n",
    "    \n",
    "    pub_urls = dict()\n",
    "    \n",
    "    try:\n",
    "        conn = get_conn_rds()\n",
    "        cursor = conn.cursor()\n",
    "        select_query = \"select * from public.publication_urls where is_scraped = 'False'\"\n",
    "        cursor.execute(select_query)\n",
    "        print(\"Selecting is_scraped false rows from the table using cursor.fetchall\")\n",
    "        result_set = cursor.fetchall() \n",
    "   \n",
    "        for row in result_set:\n",
    "            pub_urls[row[0]]= row[5]\n",
    "            \n",
    "       \n",
    "    except (Exception, psycopg2.Error) as error :\n",
    "        print (\"Error while fetching data from PostgreSQL\", error)\n",
    "\n",
    "    finally:\n",
    "    #closing database connection.\n",
    "        if(conn):\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            print(\"PostgreSQL connection is closed\")\n",
    "    \n",
    "    return pub_urls\n",
    "\n",
    "\n",
    "result_dict = select_un_scraped_publication_urls()\n",
    "for key, value in result_dict.items():\n",
    "    print(f\"{key} : {value}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility methods for STEP 4\n",
    "## 4.b\n",
    "## date_created, date_updated are set to default CURRENT_TIMESTAMP. No need to pass them in inserts anymore.\n",
    "\n",
    "def insert_pub_review_paragraphs(pub_id, pub_text, username):\n",
    "    try:\n",
    "        conn = get_conn_rds()\n",
    "        cur = conn.cursor()\n",
    "        query_params_list = {'pub_id' : pub_id,\n",
    "                             'paragraph_text': pub_text,                             \n",
    "                             'username': username}\n",
    "        sql = '''\n",
    "                INSERT INTO public.\"pub_review_paragraphs\"(pub_id, paragraph_text, username)\n",
    "                        VALUES (%(pub_id)s, %(paragraph_text)s, %(username)s);\n",
    "                '''\n",
    "        \n",
    "        cur.execute(sql, query_params_list)\n",
    "\n",
    "        conn.commit()\n",
    "        \n",
    "        print(\"Successfully inserted a record\")\n",
    "\n",
    "        cur.close()\n",
    "        \n",
    "        print(\"Closing Connection.\")\n",
    "    \n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## No need to pass in date_updated while UPDATE command. A trigger is setup for three tables.\n",
    "## so now you don't have to always remember to update the time stamps. You can just let PostgreSQL handle it for you.\n",
    "\n",
    "def set_publication_urls_is_scraped(pub_id, is_scraped, username):\n",
    "    try:\n",
    "        conn = get_conn_rds()\n",
    "        cur = conn.cursor()\n",
    "        query_params_list = {'pub_id' : pub_id,   \n",
    "                             'is_scraped': is_scraped,                                                      \n",
    "                             'username': username}\n",
    "        sql = '''\n",
    "                UPDATE public.\"publication_urls\"\n",
    "                        set is_scraped = %(is_scraped)s, username= %(username)s where id = %(pub_id)s;\n",
    "                '''\n",
    "        \n",
    "        cur.execute(sql, query_params_list)\n",
    "\n",
    "        conn.commit()\n",
    "        \n",
    "        print(\"Successfully updated a record\")\n",
    "\n",
    "        cur.close()\n",
    "        \n",
    "        print(\"Closing Connection.\")\n",
    "    \n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4 - tested it out on one url. And this method will change to fetchall from publication_urls table\n",
    "#          and extract <p> tags for paragraphs on each of the pub_url,\n",
    "#          i.e publication site and the extracted paragraphs will be inserted into the new database table: \n",
    "#          pub_review_paragraphs. \n",
    "#          After all the pub_urls for specific pub_id (id from publication_urls table) is scraped, we set is_scraped to True within publication_urls - to track completion.\n",
    "#          We don't need to track is_scraped on site_review_urls table as we're only interested in the \n",
    "#          count for publication urls scraped for tracking purpose. just as a stat for training data.\n",
    "#          Process goes on until all are completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Connecting to the PostgreSQL database...\nSelecting is_scraped false rows from the table using cursor.fetchall\nPostgreSQL connection is closed\nConnecting to the PostgreSQL database...\nSuccessfully inserted a record\nClosing Connection.\nConnecting to the PostgreSQL database...\nSuccessfully inserted a record\nClosing Connection.\nConnecting to the PostgreSQL database...\nSuccessfully inserted a record\nClosing Connection.\nConnecting to the PostgreSQL database...\nSuccessfully inserted a record\nClosing Connection.\nConnecting to the PostgreSQL database...\nSuccessfully inserted a record\nClosing Connection.\nConnecting to the PostgreSQL database...\nSuccessfully inserted a record\nClosing Connection.\nConnecting to the PostgreSQL database...\nSuccessfully inserted a record\nClosing Connection.\nConnecting to the PostgreSQL database...\nSuccessfully inserted a record\nClosing Connection.\nConnecting to the PostgreSQL database...\nSuccessfully inserted a record\nClosing Connection.\nConnecting to the PostgreSQL database...\nSuccessfully inserted a record\nClosing Connection.\nConnecting to the PostgreSQL database...\nSuccessfully updated a record\nClosing Connection.\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-04450024b526>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mpub_links_scrape_review\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-04450024b526>\u001b[0m in \u001b[0;36mpub_links_scrape_review\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pub_url errored out: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpub_url\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" and is_scraped\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_scraped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mset_publication_urls_is_scraped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpub_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_scraped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Srini'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "def pub_links_scrape_review():\n",
    "    pubid_puburls = select_un_scraped_publication_urls() \n",
    "    for pub_id, pub_url in pubid_puburls.items():\n",
    "        is_scraped = False\n",
    "        try:\n",
    "            response = requests.get(pub_url)\n",
    "            html = response.text\n",
    "            soup = BeautifulSoup(html, \"html5lib\")\n",
    "            for ptag in soup.find_all('p'):\n",
    "                if len(ptag.text) > 100:  #adding paragraph texts that atleast have 100 chars.\n",
    "                    insert_pub_review_paragraphs(pub_id, ptag.text, 'Srini')\n",
    "            is_scraped = True                        \n",
    "        except:\n",
    "            print(\"pub_url errored out: \"+pub_url+\" and is_scraped\",is_scraped)\n",
    "        set_publication_urls_is_scraped(pub_id, is_scraped, 'Srini')\n",
    "        time.sleep(30)        \n",
    "\n",
    "        \n",
    "pub_links_scrape_review()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}